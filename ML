numpy
	稀疏矩阵
		定义
			在矩阵中，若数值为0的元素远远多于非0元素的数目，并且非0元素分布没有规律时，则称该矩阵为稀疏矩阵。 稀疏矩阵只保存非零元素并假设剩余元素的值都是零，这样能节省大量的计算成本
		矩阵稀疏性量化：
			矩阵的稀疏性可以可以用一个分数来量化，即矩阵中零元素的个数除以矩阵中元素的总数

		稀疏矩阵的使用
			表示和使用稀疏矩阵的解决方案是使用替代的数据结构来表示稀疏矩阵。零元素可以被忽略，只有稀疏矩阵中的非零元素值需要被存储或使用。

		使用数据结构构造稀疏矩阵
			字典：一个字典使用行和列索引映射出一个值
			列表的列表： 矩阵的每一行都以列表形式存储，每个子列表包含列的索引和值
			坐标列表： 元祖列表存储在包含行索引、列索引和其值的每个元素在中
			

		优点
			大幅降低计算量
		缺点
			稀疏矩阵会导致空间和时间复杂度方面的问题
				空间复杂度
					大型矩阵需要大量的内存存储
				时间复杂度
					假如非常大型的稀疏矩阵可以存储在，之后将在这个矩阵上执行一些操作。简单来说，如果矩阵主要包含的是零值，即没有多少数据，那么对这个矩阵执行操作可能需要花费很长的时间，其中执行的大部分计算将、
					涉及零值相加或相乘

					这样的问题在使用线性代数的方法是浪费的，因为大多数算术运算致力于求解方程组和矩阵求逆涉及的零操作数

					矩阵运算的时间复杂度随着矩阵大小增加而增加。对于机器学习而言，即使是最简单的方法也可能需要对每一行、每一列甚至整个矩阵进行许多操作运算，这会导致执行时间变得很长。

		使用领域
			数据准备
				独热编码： 用于将分类数据表示为稀疏二元向量
				计算编码： 用于表示文档词汇中的单词的评率
				TF-IDF编码： 用于表示词汇表中词频你文档频数

			研究领域：
				处理文本文档的自然语言处理
				用于处理目录中的产品使用的推荐系统
				处理包含大量黑色像素图像时的计算机视觉问题

	稠密矩阵： 若非0元素数目占大多数时，则称该矩阵为稠密矩阵。

	vecotrize
		Numpy的vectorize类将一个函数转换成另一个函数，这个函数能把某个操作应用在数组的全部元素或切片上。值得注意点是，vectorize本质上是在对所有元素循环执行某个操作，所以并不会提升性能。
		此外，使用Numpy数组，可以对两个维度不同的数组执行操作(广播方法)

	计算矩阵的秩
		矩阵的秩就是有他的列或者行展开的向量空间的维数。
		计算方法：
			num = np.linalg.matrix_rank(numpy.array)
				参数： numpy.array 类型的数组
				返回值：返回该数组的维度数

	计算矩阵行列式
		计算方法：
			num = np.linalg.det(numpy.array)
				参数：numpy.array 类型的数组
				返回值：返回该数组的行列式值

	获取对角线上的元素
		diagonal(offset) 方法，Numpy的diagonal很容易获取矩阵对角线元素，可以通过设置offset参数在主对角线的上下偏移，获取偏移后的对角线方向的元素
			返回值：返回数组随机噢凹陷行

	计算矩阵的迹
		矩阵的迹是对角线上的元素之和，常被用在机器学习方法的底层计算中。给定一个Numpy的多维数组，使用trace就能计算出他的轨迹。
		trace()

	计算特征值和特征向量
		eigenvalues，eigenvectors = np.linalg.eig(array.numpy)
			参数： array.numpy 传入array.numpy类型的矩阵
			返回值： eigenvalues: 特征值，eigenvectors： 特征向量

	计算点积
		num 
		= np.dot(a, b)
			参数： a： 矩阵a， b： 矩阵b
			返回值： num： 返回两个矩阵计算点积之后的值



数据整理
	在整理数据时，常用的数据结构是 “数据帧” 它既直观又灵活。数据帧是呈网格状的。数据帧是用行和列来表示数据的。 
	在数据帧中有两点需要注意：
		1. 在一个数据帧中，每一行都对应一个观察值，每一列对应一个特征
		2. 每一列有一个数据头，每一行有一个索引。这些数据能用于选择及操作观察值与特征

	浏览数据帧
		使用loc或iloc能选择一个或多个数据， 也能选择一行或多行数据
		数据帧的索引不必非得是数值型。只要哦某一列在数据帧中每一行的值都是唯一的，就可以将其设置为索引。
		pandas的数据帧中所有的行都会有一个唯一的索引值。默认情况下，这个索引是一个整数，它标明了这一行在数据帧中的行的位置。然而不一定必须是这样的一个整数。数据帧的索引可以被设置成一个唯一的字母与数字组成的字符串或自定义
		数字。为了能选择一行或者部分行。pandas提供了两个方法：
			1. 当数据帧的索引是一个标签时(例如：字符串)，loc比较常用
			2. iloc并不是根据索引来查找数据的，而是根据行号来查找的，行号从0开始，逐次加1

	根据条件语句来选择行
		根据某个条件语句来选择数据帧的行数据。 有条件的筛选数据是数据整理中最常见的任务之一。很少会遇到需要使用所有的原始数据的场景，一般只是使用其中的一部分原始数据

	替换值
		replace
		可以通过Dataframe对象在整个数据帧中查找和替换值，而不仅限于在单个列中查找和替换值
		replace也接受正则表达式

	命名列
		rename
		rename可以传入一个字典作为参数。用字典可以同时改变多个列名

	计算最小值，最大值，总和，平均值，计数值，方差，标准差，峰态，偏态，平均值标准误差， 众数， 中位数， 
		max(): 最大值
		min(): 最小值
		mean(): 平均值
		sum(): 总数
		count(): 计算值
		var(): 方差
		std(): 标准差
		kurt(): 峰态
		skew(): 偏态
		sem(): 平均值标准误差
		mode(): 众数
		median(): 中位数

	查找唯一值
		unique(): 查看唯一值
		value_counts(): 显示所有的唯一值以及他们出现的次数
		nunique(): 统计有多少个唯一值
		unique和value_counts都可以用来处理和探索分类型数据的列。在数据整理阶段，分类型数据的列中经常会有一些分类需要处理。

	缺失值处理
		isnull 和 notnull 都鞥呢返回布尔类型的值类表示一个值是否缺失
		在数据整理中，缺失值是很常见的问题，pandas使用numpy的NaN来表示缺失值。但是pandas没有实现NaN。 如果要使用NaN，需要先导入numpy库，使用numpy.nan。有时候一个数据会使用特殊的值来表示缺失的观察值

	删除一列
		要删除一列，最好的方式是使用drop，并传入参数axis = 1(坐标轴列)
		drop是删除一列的常用方法。还有一种方法是 del dataframe["列名"]。 
		最好养成一种习惯，永远不要使用pandas的inplace=True参数，pandas的很多方法都包含一个inplace参数，当其被设置成True时，会直接修改数据帧本身。在对数据进行一系列更复杂的操作时，可能会出现问题，因为数据帧会被
		视为可变对象。而通过创建一个新对象的方式返回一个不可变对象会省去很多麻烦

	删除一行
		使用drop方法是个不错点的选择，只需要将传入drop方法的axis=0即可，而更实用的方式是插入布尔条件，利用条件语句来删除一行或一次性删除多行

	删除重复行
		drop_duplicates
			drop_duplicates默认只删除那些所有列都完美匹配的行。如果需要删除制定列重复的行，需要设置参数subset='列名'

	根据值对行分组
		groupby需要和一些作用于组的操作配合使用。 

	按时间段分组
		resample 按时间段对行进行分组
		resample要求索引的类型必须是datetime的值。使用resample可按一组时间间隔(偏移)来对行进行分组，然后计算每个时间组的某个统计量

	遍历一个列的数据
		apply 对一列的所有元素应用一个内置或者自定义的函数
		在数据清洗和数据整理中，apply是一个功能非常强大的函数。通常需要先洗衣歌函数来执行某个有用的操作，然后对一列中的所有元素应用这个函数

	连接多个数据帧
		concate([a, b], axis) 连接两个数据帧
			参数： a： 需要连接的a数据帧  b： 需要连接的b数据帧
				  axis： 如果axis=0 表示沿着行的方向连接数据帧 如果 axis=1 表示沿着列的方向连接数据帧

	合并两个数据帧
		要进行等值连接，就需要使用merge并用on参数指定哪些列要合并，合并是指找出并连接的两个数据帧中被合并的列中都有的行数据
		merge默认是进行行连接，如果要进行列连接，可以通过how参数进行指定  通过指定how="left" 后者 how="right" 来确定是通过左连接还是右连接

		对于任何merge的操作，都需说明三个方向的信息：
			1. 必须指定想要合并哪两个数据帧。
			2. 必须指定要根据哪两列实施合并，也就是说在两个数据帧总需要共享那一列
			3. 合并操作的类型，有how参数指定，merge支持四种主要的连接类型
				1. inner： 只返回指定列的值在两个数据帧中都存在的行
				2. outer： 返回两个数据帧的所有行。如果某一行只在其中一个数据帧中存在，就用NaN来填充缺失值
				3. left： 返回左数据帧的所有行。对于右数据帧，只返回在左数据帧中能找到匹配值的行。用NaN来填充缺失的值
				4. right： 返回右数据帧的所有行，对于左数据帧，只返回在右数据帧中能找到匹配的值，用NaN来填充缺失的值


处理数值型数据
	特征的缩放
		使用scikit-learn的MinMaxScaler来缩放一个特征数组
		机器学习中，缩放是一个很常见的预处理任务。最常见的范围是[0, 1] 或 [-1, 1] 用于缩放的方法很多，最简单的是min-max。 min-max缩放利用特征的最小值和最大值，将所有特征都缩放到同一个范围中，计算公式为：
			xi' = （xi - min(x)) / (max(x) - min(x))
			参数：
				x: 特征向量
				xi: x中的一个元素值
				xi': 是缩放后的元素值

		scikit-learn的MinMaxScaler支持两种方式来缩放特征。
			第一种方式是使用fit计算特征的最小值和最大值，然后使用tansform来缩放
			第二种方式是使用fit_transform一次性执行第一种方式所说的两个操作。

	特征标准化
		使某个特征的平均值为0，标准差为1
		scikit-learn的StandardScaler能进行标准化
			公式：
				xi' = (xi - x_) / &
				xi'是xi标准化后的形式。转换后的特征表示原始值距离平均值多少个标准差
			如果数据中存在很大的异常值，可能会影响特征的平均值和方差，也会对标准化的效果造成不良影响。在这种情况下，使用中位数和四分位数间距进行缩放会更有效，在scikit-learn中，具体做饭是调用RobustScaler

	归一化观察值
		对观察值的每个特征进行缩放，拾起拥有一致的范数(总长度为1)
		使用 Normalizer并指定norm参数 Normalizer可以对单个观察值进行缩放，使其拥有一致的范数(总长度为1)。当一个观察值有多个特征时，经常使用这种类型的缩放
		Normalizer提供三个范数选项，默认值式欧式范数

	生成多项式和交叉特征
		使用PolynomialFeatures(degree=2, include_bias=Flase)
		参数： degree 决定多项式的最高阶数
		默认情况下，PloynomialFeatures包含交互特征 通过设置interaction_only=True，可以强制创建出来的特征只包含交互特征

	特征转换
		对一个或多个特征进行自定义转换
			在scikit-learn中使用FunctionTransFormer对一组特征应用一个函数，用法和效果和pandas的apply方法一样

	识别异常
		识别异常即识别样本中的一些极端观察值(异常值)常用的方法是假设数据时正太分布的，给予这个假设，在数据周围"画"一个椭圆，将所有处于椭圆中的观察值视为正常值(标为1)，将所有处于椭圆外的观察值视为异常值(标为-1)
		查看观察值是识别异常值的方法之一，除了查看观察值，还可以只查看某些特征，并使用四分位差(IQR)来识别这些特征的极端值。 IQR是数据集的第1个四分位数和第3个四分位数之差。可以将IQR视为数据集中大部分数据的延展距离，
		而异常值会远远地偏离数据较为集中的区域。异常值尝尝被定义为比第1个四分位数小1.5IQR的值，或比第3个四分位数大1.5IQR的值

		实际上没有一个通用的识别异常值的解决方法。每一种技术都有它的优缺点。最好的策略是尝试多种技术并从整体上看结果

	处理异常值
		通常有三种方案
			1. 直接丢弃异常值
			2. 将他们标记为异常值，并作为数据的一个特征
			3. 对有异常值的特征进行转换，降低异常值的影响

		处理异常值没有一个绝对的标准。应该基于两个方面来考虑
			1. 要弄清楚是什么让他们成为异常值的。如果认为他们是错误的观察值，比如他们来自一个坏掉的传感器或者是被记错了，那么就丢弃它们或者中NaN来替代异常值。但是，如果认为这些异常值真的就是极端值，那么把他们标记为
			   异常值或者对它们的值进行转换，是更合适的方法
			2. 应该给予机器学习的目标处理异常值。

	将特征离散化
		将一个数值型特征离散化，分到多个离散的小区间
		根据数据离散化的方式，有两种方式可以使用
			1. 根据阈值将特征二值化
			2. 根据多个阈值将数值型特征离散化

		如果有足够的理由认为某个数值型特征应该被视为一个分类特征，那么离散化会是一个卓有成效的策略

	使用聚类的方式将观察值分组
		对观察值进行聚类操作，使相似的观察值被分为一组
		如果有K个分组，可以使用K_Means(K均值)聚类法将相似的观察值分到一个组，并输出一个I新的特征，以标识观察值属于哪一组

	删除带有缺失值的观察值
		大多数机器学习算法不允许目标值或特征数组中存在缺失值。因此不能简单的忽略数据中的缺失值，而是要在处理预处理阶段解决这个问题
		简单的解决方法是删除所有含缺失值的观察值，用Numpy或者pandas。 但是直接删除缺失值容易让算法丢失观察值中那些非缺失值的信息，所以删除观察值智能作为最终别无选择时迫不得已的选择
		删除观察值可能会在数据中引入偏差，这主要是由于缺失值的成因决定的，缺失值一共有三种类型
			1. 完全随机缺失
				数据缺失的可能性与任何其他东西无关
			2. 随机缺失
				数据缺失的可能性不是完全随机的，与已经存在的其他特征有关
			3. 完全非随机缺失
				数据缺失的可能性完全是非随机 的，并且与为在特征中反映出的信息有关。

		如果观察值是MCAR或者MAR，那么有时候删除观察值是可以接收到。但是如果他们是MNAR，那么数据缺失本身就是一个信息。删除MNAR观察值会导致数据产生偏差，因为这些观察值是由未观察到的系统效应产生的

	填充缺失值
		如果数据量不大，可使用KNN算法来预测缺失值
		填充缺失值可以使用scikit-learn的Imputer模块，用特征的平均值、中位数、或者众数来填充缺失值，不过效果通常会比使用KNN的差

		填充缺失值的策略主要有两种：
			1. 使用机器学习预测缺失值。为了达到目的，可以将带有缺失值的特征当作一个目标向量，然后使用剩余的特征来预测缺失值，最流行的选择是使用KNN。KNN的不足是，为了知道哪些观察值离缺失值近，需要计算每个观察值
			   与缺失值之间的距离。对于小数据集这么做没有问题，但是如果数据集中有成千上万个观察值，计算量将是一个很严重的问题。
			2. 一个比较容易扩展到大数据集的替代方案是使用平均值来填充缺失值。 可以使用scikit-learn使用特征的平均值来填充缺失值。尽管这样做效果通常没有使用KNN好，但是， "平均填充策略" 很容易扩展到包含成千上万观察值
			   的大数据集



处理分类数据
	本身没有内在顺序的类别称为nominal，，如果一组分类天然拥有内在顺序性，它就被称为ordinal
	分类信息常用一个字符串型向量或列表示，但大部分机器学习算法都要求其输入数值类型的数据

	对nominal型分类特征编码
		使用one-hot编码
			在one-hot编码中，每个分类被当做一个特征，如果特征存在就用1表示，否者用0表示。 元数据中有几个特征，就返回几个二元特征(one-hot编码的返回值即二元特征)。 通过one-hot编码，能得到观察值在一个分类中的属性值
			，并保留 分类没有层级(顺序)的状态

			在one-hot编码之后，最好从结果矩阵中删除一个one-hot编码的特征，以免线性依赖

	第ordinal分类特征编码
		对那些用机器学习的特征进行编码时，需要将ordinal分类抓换成数值，同时保留其顺序，最常见的方法就是，创建一个字典，将分类的字符串标签映射为一个数字，然后将其映射在特征上
		有一点很关键，就是只有知道ordinal分类的顺序信息，才能决定每个分类对应的数值，需要注意的是映射之间的数值

	对特征字典编码
		将一个字典转换成特征矩阵
			from sklearn.feature_extraction import DcitVectorizer
				默认情况下，DictVectorizer会输出一个稀疏矩阵来存储除0意外的元素。如果矩阵很庞大，这么做有助于节省内存。通过指定sparse=Flase能强制DictVectorizer输出一个稠密矩阵， 但是，如果是非常庞大的特征矩阵
				就需要把sparse设置为True

	填充缺失的分类值
		如果有一个分类特征中包含缺失值，需要用预测值来填充
		最理想的方案是训练一个机器学习分类器来预测缺失值，通常会使用KNN分类器
		另一个方案是用特征中出现次数最多的值来填充缺失值

	处理不均衡分类
		处理一个分类极度不均衡的目标向量
			处理方案：
				1. 收集更多的数据，尤其是占少数的分类观察值   （最好的解决方案）
				2. 改变评估模型的衡量标准， 选择更适用于不均衡数据的标准    （次优的解决方案）
				3. 考虑使用嵌入分类权重参数的模型、上采样或下采样，在下采样中，需要从占多数的分类中创建子集，其观察值与占少数的分类的观察值数量相等。在上采样中，采取有放回的方式对占少数的分类重复采样，以此创建与
				   占多数的分类有相同数量观察值的数据集。具体使用上采样还是使用下采样，看实际情况而定


处理文本数据集
	清洗文本
		对一些非法结构化的文本数据进行及基本的清洗，大部分基本的文本清洗操作用python的常用字符串操作即可完成，其中strip、replace、split用的最多，同时也可以使用正则表达式做一些复杂的字符串操作
		大部分文本数据被用于特征之前都要进行清洗。

	解析并清洗HTML
		正则表达式，xpath，beautiful soup

	移除标点
		使用字符串的translate函数


处理日期和时间
	把字符串转换成时间
		把一个代表日期和时间段的字符串向量转换成时间序列数据
			解决方案：使用pandas的to_datetime函数，并通过format参数指定字符串的日期、时间格式，如果设置errors="coerce"，当转换出现错误时并不会抛出异常(默认), 但是会将这个错误的值设置成NaT(缺失值)

	处理时区
		为一组时间序列数据添加或改变时区
			如果没有特别指定，pandas的对象都是没有时区的，不过可以在创建对象时通过tz参数指定时区
			pandas支持两种表示时区的字符串，但是建议使用pytz库的字符串。导入all_timezones库就能看到所有代表时区的字符串

	选择日期和时间
		从一组日期向量中选择一个或多个日期值
			使用两个布尔条件语句分别设置开始日期和结束日期 或者，将日期这一列设为数据帧的索引列，然后用loc进行筛选  具体使用哪种方法根据场景决定，如果想执行复杂的时间序列操作，那么将date这里一列设置为数据帧
			的索引是值得的；但是如果只想做一些简单的数据整理，那么使用布尔条件语句会更简单

	将日期数据切分成多个特征
		用一列日期和时间数据来创建 年 月 日 时 分 秒的特征
			使用pandas函数的Series.dt的时间属性

	对一周内地各天进行编码
		计算一个日期向量中的每个日期是星期几
			使用pandas中Series.dt的weekday_name属性， 如果希望输出是一个数值，以便成为一个更有用的机器学习特征，则可以是哟宁weekday属性。它用一个整数表示一个星期中的星期几

	创建一个滞后的特征
		创建一个滞后n个时间段的特征

	使用滚动时间窗口
		计算一个时间序列数据针对某个滚动时间的统计量

	处理时间序列中的缺失值
		处理时间序列数据中的缺失值
			针对时间序列数据，可以使用插值法来填充由缺失值造成的数据缺口或者通过向前填充法，使用前面的值来替换缺失值或者 通过向后填充发，使用后面的值来代替前面的值

			插值法实际上就是根据缺口附近的已知数据来画一条直线或者曲线，然后利用这条直线或者曲线预测合理的值。当时间间隔确定，数据不会产生剧烈波动且缺失值缺口比较小的时候，插值法尤其有用
			如果缺失值的缺口很大，不想对整个缺口进行插值。在这种情况下，可以司洪勇limit限制插值的数量，并用limit_direction来设置是从最后一个已知值进行向前插值，还是进行反向插值
			向后填充和向前填充都是简单的插值法，它们都是画一条经过已知值的线，并利用这条线来天哦冲缺失值。向后填充和向前填充还有一个小优点，即不需要知道缺失值两侧的值


图像处理
	平滑处理图像
		平滑处理图像就是将每个像素的值变换为其相邻像素的平均值。相邻像素和所执行的操作在数学上北大表示为一个核。核的大小决定了平滑的程度，核越大，产生的图像越平滑。
		方法： cv2.blur(image, ksize)
			参数： image：被平滑的图像 ksize： 平滑的核的大小

		在图像处理中，核被广泛应用于从图像锐化到边缘检测的所有领域
		核中的中心元素是要处理的像素，而其余元素是该像素的相邻像素。由于所有元素具有相同的值(被归一化为1)。因此每个元素对要处理的像素点有相同的权重。可以使用filter2D在图像上手动应用核，以产生平滑效果

	图像锐化
		创建一个能突出显示目标像素的核，然后使用filter2D将其应用于图像
		锐化的原理和平滑类似，在平滑处理中，使用核来平均处理相邻像素的值；而在锐化时，使用的是能突出现实像素自身的核。锐化可以使图像的边缘更加突出

	创建对比度
		直方图均衡是一种图像处理的方法，他可以使图像中的物体和形状更加突出。对于灰度图像，可以直接在图像上应用OpenCV的equalizeHist。但是，如果对彩色图像进行增强操作，首先需要将其转换为YUV格式。其中Y表示亮度
		U和V表示颜色。转换后，可以将equalizeHist方法应用于图像，然后将其转换回BGR或RGB
		经过直方图均衡后得到的图像往往看起来不 "真实", 但是，图像只是底层数据的可视化表示。如果直方图均衡能使我们感兴趣的对象与其他对象或背景区分得更明显，那么它对图像预处理流水线来说是一个有价值的补充

	颜色分离
		在OpenCV中，分离颜色很简单。首先将图像转换为HSV格式(H、S、V 分别代表色调、饱和度、亮度)。其次，定义要分离的一系列值，这部分工作可能是最困难和最耗时的。然后，为图像创建一个掩模(只保留掩模的白色区域)
		最后，使用bitwise_and方法将掩模应用于图相关，并将图像转换为所需要输出的格式

	图像二值化
		图像二值化是指对图像进行阈值处理，即将强度大于某个阈值的图像设置巍峨白色并将阈值小于改制的像素设置为黑色的过程。还有一个更先进的技术，叫作自适应阈值处理，在这种方法中，一个像素的阈值是由其相邻像素的强度决定的
		当图像中不同区域的光照条件有差异时，用这种方法处理会很有帮助

		自适应阈值处理
			adaptiveThreshold(image, max_output_value, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, neighborhood_size, subtract_from_mean)
				参数： 
					  image： 需要处理的图像
					  max_output_value： 确定输出像素的最大强度
					  ADAPTIVE_THRESH_GAUSSIAN_C： 将像素的阈值设置为相邻像素强度的加权和，其权重有高斯窗口确定
					  neighborhood_size： 块的大小(用于确定像素阈值的领域大小)
					  subtract_from_mean: 用来手动调整阈值的常熟

		对图像进行阈值处理的一个主要优点是可以对图像去噪，保留重要的元素。


	移除背景
		提取图像中的前景图像
			在所需要的前景图像周围画一个矩形，然后运行GrabCut算法
			cv2.grabCut(image_rgb, mask, rectangle, bgdModel, fgdModel, 5, cv2.GC_INIT_WITH_RECT)
				参数：
					  image_rgb: 图像
					  mask：掩模
					  rectangle：矩形
					  bgdModel：背景的临时数组
					  fgdModel：前景临时数组
					  5：迭代次数
					  cv2.GC_INIT_WITH_RECT： 使用定义的矩形初始化

		grabCut做的非常好，但是图像中仍然残留部分背景区域。可以手动将这些区域标记为背景，但是，如果在实际工作中，如果图像过多，可以考虑接受保留部分背景噪声图像
		grabCut原理：
			首先在包含前景的区域画成一个矩形。GrabCut认为这个举行之外的都是背景，并使用这些信息找出矩形内可能的背景。算法最后会生成一个掩模，他可以标出确定为背景的区域、可能为背景的区域和前景区域

	边缘检测
		方法：
			cv2.Canny()

		边缘检测可以帮助我们祛除低信息含量的区域，并将信息含量最高的的区域从原始图像中分离出来
		Canny边缘检测器需要两个参数来表示梯度的低阈值和高阈值。处于低阈值和高阈值之间的潜在边缘像素被认为是弱边缘像素，而高于阈值的则被认为是强边缘像素。在解决方案中，将这两个阈值分别设置低于和高于像素强度中位数
		一个标准偏差的值。如果在对整个图像集运行Canny之前，先用一小部分图像试错，手动调整阈值，通常会得到更好的处理结果

	角点检测
		方法：
			cv2.cornerHarris()

		OpenCV的Harris角点检测器是检测两条边缘线条的交叉点的常用方法。Harris检测器会寻找一个窗口(也可以叫领域)。 这个窗口的微小移动会引发窗口内像素值的大幅变化。cornerHarris包含三个重要的参数，block_size代表
		角点检测中窗口的尺寸，aperture代表Sobel算子的尺寸，free_parameter用于控制对角点检测的严格程度，这个值越大，可以识别的角点越圆滑。然后，师兄阈值筛选出最可能的角点，或者使用Shi-Tomasi角点检测器确定一组
		固定数量的明显角点。其工作方式与Harris检测器类似。gooFeaturesToTrack有三个主要参数：待检测点的数量、角点的最差质量以及角点之间的欧氏距离

	为机器学习创建特征
		使用numpy的flatten方法将包含图像信息的多维数组转换成包含样本值的特征向量



利用特征提取进行特征降维
	在一组数据帧中，并非所有的特征都是同样重要的，通过特征提取进行降维的目的就是对原特征集进行变换，变换后得到新的特征集，新的特征集保留了大部分信息。换句话说，特征变换就是通过牺牲一小部分数据信息来减少特征的数量，
	并保证还能做出准确的预测

	使用主成分进行特征降维
		主成分分析法是一种流行的线性降维方法。PCA将样本数据映射到特征矩阵的主成分空间(主成分空间保留了大部分的数据差异，一般具有更低的维度)。 PCA是一种无监督的学习方法，也就是说只考虑特征矩阵而不需要目标向量的信息
		方法:
			from sklearn.decomposition import PCA
			pca = PCA(n_components=0.99, whiten=True)
				参数：
					  n_components： 如果该值大于1，将返回和这个值相同的数量的特征，如果该值在0-1之间，pca就会返回维持一定信息量(方差代表一定信息量)的最少特征数。通常情况下n_components值取为0.95或0.99.以为这保留
					                 95%或者99%的原始特征信息。
					  whiten=True：  表示对每个主成分都进行转换以保证他们的平均值为0、方差为1.
					  另一个参数是svd_solver="randomized"，代表使用随机方法找到第一个主成分(这种方法通常速度很快)

	对线性不可分数据进行特征降维
		如果数据是线性不可分的(例如：只能用曲线边界分类的数据),那么线性变幻的效果不会很好。可以使用make_circles方法生成一个模拟数据集，有一个拥有两个分类和两个特征的目标向量。make_circles生成的就是线性不可分数据
		其中一个分类从外部完全包围。如果使用线性PCA对数据进行降维，则两类数据会被线性映射到一起。理想情况下，希望得到的结果是既能降低数据的维度，又可以使数据变得线性可分

	通过最大化类间可分性进行特征降维
		对特征进行降维操作，然后将其应用于分类器
			使用线性判别分析方法，将特征数据映射到一个可以使类间可分性最大的成分坐标轴上
			LDA是一种分类方法，也是常用的降维方法。LDA和PCA的原理类似，它将特征空间映射到较低维空间。PCA只需要关注是数据差异变化大的成分轴，在LDA中，另一个目标是找到使得类间差异最大的成分轴。
			实现方法：
				from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
				lad = LinearDiscriminantAnalysis(n_components=1)
				features_lad = lad.fit(features, target).transfrom(features)

				在scikit-learn中，LDA是有LinearDiscriminantAnalysis方法实现的，其中包含一个参数n_components，表示需要返回的特征数量

	使用矩阵分解法进行特征降维
		使用非负矩阵分解法对特征矩阵进行降维
		NMF是一种无监督的线性降维方法，它可以分解举证(将特征矩阵分解为多个矩阵，其中乘积近似于原始矩阵)，将特征矩阵转换为表示样本与特征之间ianzai关系的矩阵。简单的说NMF可以减少维数，因为在矩阵乘法中，两个银子的维数要比
		得到的乘积维数低很多。正式的，给定一个期望的返回特征的数量，NMF将把特征矩阵分解为 V≈WH其中，V是d*n维特征矩阵(即d个特征，n个样本)，W为d*r维矩阵，H是r*n维矩阵。通过调整值，可以设定希望减少的维数
		如果要使用NMA，特征矩阵就不能包含负数值。此外NMA不会告诉我们输出特征中保留了原始数据的信息量，因此找出参数n_components的最优值的最佳方法是不断尝试一系列可能的值，知道找到最佳学习模型的值

	对稀疏数据进行特征降维
		对稀疏特征矩阵进行特征建委操作，采用截断奇异值分解TSVD
			TSVD与PCA类似，事实上PCA尝尝在某一个步骤中使用非截断奇异值分解(SVD)法。在常规SVD中，对于给定的d个特征，SVD将创建dxd维银子矩阵，而TSVD将返回nxn维因子矩阵，其中n是预先指定的参数。与PCA相比，TSVD的优势在于
			他适用于稀疏特征矩阵。但是TSVD有一个问题，其输出值的符号会在多次拟合中不断变化(这是由其使用随机数生成器的方式决定的)。一个简单的解决方法是对每个预处理管道只使用因此fit方法，然后多次使用transform方法
			与LDA一样，TSVD需要通过参数n_components指定想要的特征(分数)数。 选择特征数最优的一种方法是通过超参数n_components作为超参数进行优化(即选择产生最佳训练模型的n_components值)由于TSVD提供了每个成分保留的原始
			特征矩阵信息的比例，因而可以按照要保留的信心量(常用的值是95%或99%)选择成分


使用特征选择进行降维
	特征选择：特征选择会保留信息量较高的特征而丢弃信息量较低的特征
	特征选择的三个方向： 过滤器、包装器、嵌入式方法

	数值型特征方差的阈值化
		从一组数值型特征中移除方差较小(即可能包含的信息较少)的特征
		方法：
			from sklearn.features_selection import VarianceThreshold
			thresholder = VarianceThreshold(threshold=.5)
		方差阈值化是最基本的特征选择方法之一。这种方法的依据是小方差的特征可能比大方差的特征的重要性更低一些。 VT方法的第一步是计算每个特征的方差。算出方差后，方差低于阈值的特征会被丢弃。采用VT方法是需要主要两点：
		1. 方差不是中心化的(它的单位是特征单位的平方)。因此如果特征数据集中特征的单位不同，那么VT法就无法起到作用。
		2. 方差的阈值是手动选择的，所以必须依靠人工来选择一个合适的阈值。
		如果特征阈值已经标准化，方差阈值将起不到作用

	二值特征的方差阈值化
		二值特征方差阈值化即特征数据只有两个分类，移除其中方差较小的特征
		方法：
			from sklearn.features_selection import VarianceThreshold
			threshold = VarianceThreshold(threshold=(.75 * (1 - .75)))
		和数值型特征一样，挑选高信息的分类特征的方法之一就是查看他们的方差。在二值特征中，方差的计算公式为： Var(x) = p(1-p) 其中，p是观察值属于第一个分类的的概率。通过设置P值，可以删除大部分观察值都属于同一个类别的特征

	处理高度相关性的特征
		使用方差矩阵检查是否存在较高相关性的特征，如果存在，则删除其中一个
		在机器学习中经常会遇到特征高度相关的问题。如果两个特征高度相关，那么他们所包含的信息就非常相似，因此这两个特征就存在冗余。解决方案是从特征集中删除一个与其他特征高度相关的特征即可

	删除与分类任务不相关的特征
		根据分类的目标向量，删除信息量较低的特征
		对于分类型特征，计算每个特征和目标向量的卡方统计量

	递归式特征消除
		石英管scikit-learn的RFECV类通过交叉验证进行递归式特征消除。该方法会反复训练模型，每次训练移除一个特征，知道模型性能变差。剩余的特征就是最优特征


模型评估
	交叉验证模型
		交叉验证缺点：
			1. 模型的性能高度依赖于所选取的测试集
			2. 模型在训练和评估时都没有充分利用所有的可用数据

		解决方案：
			K折交叉验证(KFCV)
			在K折交叉验证中，数据被分成k份，训练模型时需要将k-1分数据组合起来作为训练集，剩下的一部分数据作为测试集。将上述过程重复k次，每次去一份不同的数据作为测试集，对模型在k次迭代中的得分取平均值作为总体得分
			在使用KFCV时需要考虑三点问题：
				1. KFCV假定每个样本都是独立于其他样本的(即数据是独立同分布的)。如果数据是独立同分布的，最好在数据分组前将顺序打乱。在sklearn-learn中，可以通过设置shuffle=True来将数据顺序打乱
				2. 使用KFCV时通常将每类数据大致平分到k组数据中(这被称为分层k折)
				3. 使用Hold-out验证或交叉验证时，应该基于训练集对数据进行预处理，然后将这些预转换同时应用于训练集和测试集，这非常重要

	评估二元分类器
		使用scikit-learn的cross_val_score 方法进行交叉验证，同时使用scoring参数来决定性能评估指标，可以在准去率、精确度、召回率和F1分数等多种指标中选择 

		准确率： 是一种常见的性能指标，他表示被正确预测的样本占参与预测的样本总数的比例，计算公式为   准确率 = （TP + TH）/ （TP +  TN + FP  + FN）
				其中： TP是"真阳性"的数量，代表属于正类，而且被正确预测的岩本
				       TN是 "真阴性"的数量， 代表本身属于 "负类"， 而且被正确预测的样本
				       FP是"假阳性"的数量，也被称为I类错误，代表本来属于负类，却被错误预测为正类的样本
				       FN是"假阴性"的数量，也称为II类错误，代表本来属于正类，缺被错误的预测为负类的岩本

		精确度：是所有被预测为正类的样本中被正确预测的样本的百分比。可以把它看作一种衡量预测结果中的噪声的指标。即当样本被预测为正类时，预测正确的可能性有多大。具有高精确度的模型时悲观的，因为他们仅在非常确定
		        确定时才会预测样本为正类。 精确度计算公式为： 精确度 = TP / (TP + EP)

		召回率： "真阳性" 样本占所有正类样本的比例。召回率衡量的是模型识别正类样本的能力。召回率高的模型是乐观的，因为他们比较容易将样本预测为正类  召回率： TP / (TP + FN) 

		F1分数： 大多数情况下，希望在精确度和召回率之间达到某种平衡，而F1分数(F1 score) 可以满足这种需求。 F1分数是低矮哦和平均值(一种用于概率数据的平均数), 计算公式为： F1 =  2 *  (精确度 * 召回率) / (精确度 + 召回率)
		         F1分数是衡量正类预测的正确程度的指标，代表在被标记为正类样本中，确实是正类点的样本所占的比例

		作为评估指标，正确率包含一些有价值的特性，而且它非常易于理解。然而，好的衡量标准还要在精确度和召回率之间保持某种平衡，即模型要在乐观和悲观之间保持平衡。分数代表了召回率与精确度之间的一种平衡，两者的相对贡献是相等的

	评估二元分类器的阈值
		受试者工作特征曲线是评估二元分类质量的常用方法。 ROC曲线会对每个概率(即用来区分样本属于正类或者负类的概率值)阈值比较其真阳性和假阳性的比例。通过绘制ROC曲线，可以观察模型的性能
		ROC曲线表示每个概率值下相应的TPR和FPR值。除了能够可视化TPR和FPR之间的关系，ROC曲线还可以用作模型的通用度量标准。模型越好，曲线越高。曲线下的面积也就越大。处于这个原因，通常会计算ROC曲线下方的面积(AUCROC)来判断
		在所有可能的预知下模型的总体性能水平。AUCROC的值越接近1，模型的性能越好。

	评估多元分类器
		 如果分类数据均衡，准确率是评估模型性能的一个简单且可解释的指标。准确率是被正确预测的样本数与样本总数之比，并且在二元分类器和多元分类器中的效果一样。但是，当分类数据不均衡时，应该使用其他指标
		 使用精确度、召回率、F1分数，将数据看做一组二元分类数据的集合，使用上述指标来评估多元分类器。就能对每个分类应用二元分类器的指标，就好像它是数据中点的唯一分类一样，然后对所有分类的得分求平均值，得到总分数

	分类器性能的可视化
		混淆矩阵类比较预测分类和真实分类
		混淆矩阵是将分类器性能可视化的简单有效的方式。他的一个主要优点就是很容易解释。矩阵的每一列表示样本的预测分类，而每一行表示样本的真实分类。最终的每个单元格都是预测分类和真实分类的一种可能组合。
		混淆矩阵的注意点：
			1. 一个完美的模型，应该只有对角线上有值，而其他位置应该全为0.而给糟糕的模型，其混淆矩阵看起来就像是将样本均匀分配到了哥哥单元格上。
			2. 混淆矩阵不仅可以显示模型将哪些样本分类预测错了，还可以显示它是怎么分错的，即误分类的模式
			3. 对于任意数量的分类，混淆矩阵都适用

	
模型选择
	适用穷举搜索选择最佳模型
		GridSearchCV是一种适用交叉验证进行模型选择的暴力方法。用户为一个或多个超参数定义候选值的集合，然后GridSearchCV使用其中的每个值或值得组合来训练模型。性能得分最高的模型被选为最佳模型
		GridSearchCV的verbose参数值得注意的，虽然该参数一般不需要设置，但在长搜索过程中可以设置该参数来获取一些过程进展信息。Verbose参数决定了搜索期间输出的消息量，0表示没有输出，1到3代表输出信息，数字越大表示输出的
		细节越多。

	使用随机搜索选择最佳模型
		在用户提供的参数分布(如正太分布，均匀分布)上选取特定数量的超参数随机组合。scikit-learn使用RandomizedSearchCV实现随机搜索，它比穷举搜索更节省计算资源
		司洪勇RandoizedSearchCV时，如果指定分布，那么scikit-learn将从该分布中对超参数进行无放回的进行随机采样。在搜索结束后Ran董秘则对SearchCV使用整个数据集和最佳超参数训练一个新模型。该模型和scikit-learn中的其他
		模型一样可以用来做预测。

	将数据预处理加入模型选择过程
		使用数据训练模型之前，一般需要对数据进行预处理。在做模型选择是，必须小心地预处理数据。首先GridSearchCV使用交叉验证来确定最佳模型。然而在交叉验证中，我们假定用于测试的那一份数据在训练时是不可见的，因此，这些数据
		不应该在预处理步骤中使用。
		预处理必须是GridSearchCV操作的一部分。这在scikit-learn中很容易做到。FeatureUnion来组合两个预处理步骤：特征值标准化(StandardScaler)和主成分分析(PCA)，生成一个名为preprocess的对象。 它包含两个预处理步骤。
		然后用学习算法将preprocess一同包含到流水线中。最终的结果就是，将拟合、转换和使用各种超参数组合训练模型等这些复杂的操作全部移交个scikit-learn来处理
		一些预处理方法有自己参数，这些参数通常必须由用户提供。例如使用PCA做降维需要用户生成转换特征集的主成分数量。理想情况下，会选择能够为指定测试指标生成最佳模型的主成分数据。scikit-learn能简化这个操作。当搜索控
		空间中包含预处理参数候选值时，他们会想其他超参数一样被处理。


线性回归
	拟合一条直线
		线性回归假设特征与目标向量之间为近似线性关系。也就是说，特征对目标向量点的影响(也称为系数，权重或参数)是恒定的
		线性回归的主要优点是可解释性的，因为模型的系数代表特征每单位的变化对目标向量的影响

	处理特征之间的影响
		经常会相信两个特征之间存在相互作用，但有时并不是这样。在这种情况下，使用scikit-learn的PolynomialFeatures为所有的特征组合创建交互影响(交互特征)会很有用，然后就可以使用模型选择策略找出能产生最佳模型的特征和
		交互项组合

		要使用PolynomialFeatures创建交互特征，必须要设置三个参数。
			degree：决定最多用几个特征来创建交互特征
			interaction_only： interaction_only=True  只返回交互特征
			include_bias： include_bias = Flase 阻止加入偏差

	拟合非线性关系


树和森林
	基于树的学习算法的基础是包含一系列决策规则的决策树。第一个策略在规则的顶部，随后的决策规则在其下面展开。在决策树中，每个决策规则产生一个决策节点，并创建通向新节点的分支。终点处没有决策规则的分支别成为叶子节点


KNN 
	在寻找最近的邻居或者使用基于距离的某种学习算法时，很重要的一件事是转换特征，使所有特征采用同样的单位。这样做是因为距离指标认为所有特征的单位都是相同的
	创建一个KNN分类器
		如果数据集不是特别大，使用KNeighborsClassifier
		在KNN算法中，如果给定一个分类未知的观察值，第一步会先基于某个距离指标找到最近K个观察值，然后这K个观察值基于他们自己的分类来投票，得票最多的就是预测的分类。
		KNeighborsClassifier中的参数
			metric：设定使用何种距离指标
			n_jobs： 设定使用多少个CPU内核
			algorithm： 设定计算最近邻居 的算法

	确定最佳的领域点集的大小
		为KNN分类器找到最佳的K值
			K值得大小对KNN分类器的性能是有重要影响的。在机器学习中，一直尝试在偏差和方差之间找到一种平衡，而k值对这种平衡的影响很明显。如果k=n(n是观察值得数量)，那么偏差会很大而方差会很小。如果k=1，那么偏差会很小，但是方差
			很大。只有找到了能在偏差和方差之间取得折中的k值，才能得到最佳的KNN分类器

		创建一个机遇半径的最近邻分类器
			对于分类未知的观察值，根据一定距离范围内所有观察值的分类来确定其分类
			解决方案：
				使用RadiusNeighborsClassifier
				在KNN分类器中，观察值的分类是根据他的K个邻居的分类来预测的，而在不太常用的基于半径的最近邻分类器中，观察值的分类是根据某一半径r范围内所有观察值的分类来预测的。


逻辑回归
	逻辑回归是一种被广泛使用的二元分类器。在逻辑回归中，线性模型被包含在一个逻辑函数，sigmoid函数中。 在scikit-learn中，使用LogisticRegression学习一个逻辑回归模型，一旦被训练出来，这个模型就可以用于预测新观察值的分类

	训练多元分类器
		创建多元分类器的方式和创建二元分类器的方式一样，不同的是LogisticRegression方法中的函数添加了一个参数 multi_class="ovr", 也可以把multi_class参数设置为multinomial，改为MLR

	通过正则化来减小方差
		正则化是一种通过惩罚复杂模型来减小其方差的方法。准确的说，就是将一个惩罚项加在希望减小的损失函数上，通常为L1惩罚和L2惩罚。
		在逻辑回归中，为了减小方差，把C当作需要被调校的超参数，以寻找一个可以创建最佳模型的C值。在scikit-learn中，可以使用LogisticRegressionCV类来有效的调校C。LogisticRegressionCV中的参数CS可以接受两类值，一类是可以用来
		搜索的C的范围，另一类是一个整形数值。对于整形数值，LogisticRegressionCV会生成一个列表，其长度为这个整型数值，里面的元素是从对数空间中取得大小在-10000到10000之间的值

	超大数据集上训练分类器
		在scikit-learn中通过LogisticRegression使用随机平均梯度solver来训练一个逻辑回归模
		scikit-learn的LogisticRegression提供了不少用于训练逻辑回归莫兴国的方法，这些方法被称为solver。大部分情况下，scikit-learn会帮我们自动选择最佳solver，或者警告我们不能做某些事情

	处理不均衡的分类
		LogisticRegression自带了一个处理不均衡分类的方法。如果数据集中的分类特别不均衡，而且在数据余处理中并没有解决这个问题，就可以使用哦class_weight参数给分类设置权重，确保数据集中的各个分数是均衡的。具体的说
		balanced参数值会自动给各分类加上权重，而权重值与分类出现的评率的倒数相关。


支持向量机
	支持向量机通过寻找一个能最大化训练数据集中分类间距的超平面来给数据分类。

	训练一个线性分类器
		scikit-learn的LinearSVC实现了一个简单的SVC。 在SVC中，后者是通过一个超参数C来控制的。C是SVC学习器的一个参数，也是学习器将一个样本点分类时被施加的罚项。当C很小时，SVC会因为对数据的错误分类而被重罚，因此通过反向传播
		来避免对样本点的错误分类。 在scikit-learn中，C是由参数C决定的，默认是1.0.

	使用核函数处理线性不可分的数据
		使用径向基核函数，可以创建一个分类效果比线性核函数好很多的决策边界。
		在scikit-learn中，可以通过设置kernel参数的值来选择想用的核函数。一旦选择了一个核函数，就需要为这核函数确定一些合适的选项值

	计算预测分类的概率
		SVC算法使用一个超平面来创建决策区间，这种做法并不会直接计算出观察值属于某个分类的概率。但是可以输出校准过的分类概率，并给出几点说明。在有两个分类的SVC中可以使用Platt缩放它首先训练这个SVC，然后训练一个独立的交叉验证
		逻辑回归模型将SVC的输出转换为概率

		计算预测分类的概率有两个主要问题： 
			1. 因为还训练了一个带交叉验证过的模型，所以生成预测分类概率的过程会显著增加模型训练时间
			2. 因为预测的概率是通过交叉验证计算出来的，所以他们可能不会总是与预测的分类匹配。也就是说，一个观察值可能被预测为属于分类1，但他被预测而为属于分类1的概率却小于0.5

			在scikit-learn中，这些预测的概率必须在训练该模型时计算出来。可以通过设置SVC的probaility参数为True来做到这一点。在模型被训练完之后，可以使用predict_proba方法输出观察值为每个分类的预测概率

		识别支持向量
			超平面是由相对而言的一小部分观察值(被称为支持向量)所决定的。直观的说，可以想象成超平面是由支持向量"举起来"的。因此，这些支持向量对于模型来说非常重要。如果，如果从数据集中移除一个非支持向量的观察值，那么模型不会变
			，但是如果移除一个支持向量，超平面与分类之间的间距就不会是最大的了
			训练完SVC之后，scikit-leran提供了很多识别支持向量的选项。解决方案用support_vectors_来输出模型中观察值特征的4个支持向量。也可以使用support_来查看支持向量在观察值中的索引值


	处理不均衡的分类
		在支持向量机中，C是一个超参数，他决定这一个观察值被分错类后的惩罚。在支持向量机中处理分类数据不均衡的一个方法是，对不同的分类使用不同的权重C。这里的C是对错误分类的惩罚。



朴素贝叶斯
	贝叶斯定理是已知事件A发生的先验概率P(A), 以及事件A发生的条件下事件B发生的概率P(B|A)时，获得概率P(A|B)的重要方法：  P(A|B) = P(B|A)P(A) / P(B)
	朴素贝叶斯的优点：
		1. 方法直观
		2. 对于小样本量的数据也能起作用
		3. 进行训练和预测的计算成本低
		4. 对于各种不同的参数设置总能得到稳定的结果

		公式：  P(y|x1, ..., xj) = P(x1,...,xj|y)P(y) / P(x1,...,xj)
			参数：P(y|x1, ..., xj) 后验概率，表示一个观察值再起j个特征xi,...xj的情况下，他的分类是类别y的概率
    			   P(x1,...,xj|y)  似然概率，表示给定观察值的分类y，其特征是x1,...,yj的概率。
    			   P(y)  先验概率，是查看数据之前对于分类y出现的概率的猜测
    			   P(x1,...,xj)  叫作边缘概率

    	在朴素贝叶斯中，对观察值每个可能的分类后验概率进行比较。因为在这些比较比较中边缘概率是恒定不变的，所以只需要比价每个分类的后验概率就好。对于每个观察值，后验概率最大的分类就是这个观察值的预测分类
    	对于朴素贝叶斯分类器来说，有两个重要的地方需要指出：
    		1. 对于数据的每个特征，必须假定他的似然率P(xj|y)的统计学分布。最常用的分布有正太(高斯)、多项式分布和伯努利分布。对这些分布的选择总是由特征的特征决定的
    		2. 朴素贝叶斯得名于一个假设--每个特征和它的似然率是相互独立的。这种"朴素"的假设经常是错的，但是在实际操作中它并不影响构件一个高品质的分类器。

    为连续的数据训练分类器
    	最常用的朴素贝叶斯分类器就是高斯朴素贝叶斯分类器。

    为离散数据和计数数据训练分类器
    	多项式朴素贝叶斯分类器与高斯朴素贝叶斯分类器的工作原理类似，但是样本数据的特征为服从多项式分布。但在实际应用中，这意味着这种分类器主要用于数据时离散的情况。常见的使用多项式朴素贝叶斯分类器的场景是文本分类，
    	使用词袋或者TF-IDF方法


聚类
	聚类算法的目标是找出观察值潜在的分类，如果做的好的话，能在没有目标向量的的情况下预测观察值的分类。
	聚类算法有很多，它们使用了多种不同的方法来识别数据中的聚类。

	使用K-means聚类算法
		K-Means聚类是常见的一种聚类算法。在K-Means聚类中，算法试图把观察值分到k个分组中，每个分组的方差都差不多。分组的数量k是用户设置的一个超参数。具体来讲，K-Means算法有以下几个步骤：
			1. 随机创建k个分组(即cluster)的 "中心"点
			2. 对于每个观察值：
				1. 算出每个观察值和这k个中心点之间的距离
				2. 将观察值指派到离他最近的中心点分组
			3. 将中心点移动到相应分组的点的平均值位置
			4. 重复步骤2和3，知道没有观察值需要改变它的分组。这时该算法就被认为已经收敛，而且可以停止了
		关于K-means算法，有三点需要注意的地方：
			1. K-Means聚类假设所有的聚类是凸形的
			2. 所有特征在同一度量范围内。
			3. 分组之间的是均衡的
			如果无法满足这些假设，就可能需要尝试其他的聚类方法
		在scikit-learn中，K-Means聚类是通过KMeans类实现的。其中最重要的参数是n_clusters它设定聚类的数量k。

	加速K-Means聚类
		Mini-Bach K-Means 和K-Means算法的工作原理类似。如果不过多研究细节，那么这两种算法的区别是，Mini-Bach K-Means计算量最大的步骤只在观察值的一部分随机样本上而非所有的观察值上执行。这个方法可以在只损失一小部分
		质量的情况下显著缩短算法收敛的时间

	使用Meanshift聚类算法
		K-Means有一个缺点，就是要在训练之前设定聚类的数量k，而且还要假设聚类的形状。Meanshift算法就没有这些限制
		Meanshift有两个重要的参数。第一个是bandwidth，它设定了一个观察值用以决定移动方向的区域(又叫作核)的半径。默认情况下，MeanShift会自动估计一个合理的bandwidth值(会显著增加计算开销)。 第二，有时候，执行Meanshift
		算法时，在一个观察值的核中看不到任何其他观察值。默认情况下，MeanShift把这些所有的"孤儿观察值"分配给离他最近的观察值的核。如果想丢弃这些孤值，可以设置cluster_all=Flase。这样，所有的孤值的标签就被设定为-1

	使用DBSCAN聚类算法
		DBSCAN对于聚类的形状没有做任何假设
			1. 选择一个随机的观察值
			2. 如果xi的近邻数为最小限度数量的话，就把他归入一个聚类
			3. 对xi的所有邻居重复执行步骤2，对邻居的邻居也如此，以此类推。这些点就是聚类的核心观察值
			4. 一旦步骤3处理完所有临近的观察值，就选择一个新的随机点(重新开始执行步骤1)
		一旦这些步骤完成，就会得到一个聚类的核心观察值。最后，凡是在聚类附近但又不是核心观察值，将被认为属于这个角力，而那些离聚类很远的观察值将被标记为噪声
		DBSCN对象需要设置以下3个参数：
			eps：从一个观察值到另一个观察值最远距离，超过这个距离将不再认为二者是邻居
			min_samples：最小限度的邻居数量，如果一个观察值在其周围小于eps距离的范围内已有超过转增数量的邻居，就被认为是核心观察值
			metric：eps所用的距离度量，比如minkowski(闵可夫斯基距离)或者euclidean(欧式距离)

	使用层次合并聚类算法
		Agglomerative聚类是一个强大的，灵活的层次聚类算法。在Aggloverative聚类中，所有观察值一开始都是一个独立的聚类。接着，满足一定条件的聚类被合并。不断重复这个过程，让聚类不断增长，知道达到某个临界点。在scikit-leran
		中，AgglomerativeClustering使用linkage参数来决定合并策略。使其可以最小化下面的值： 1. 合并后的聚类方差 2. 两个聚类之间观察值的平均距离 3. 两个聚类之间观察值的最大距离
		还有两个参数affinity： 决定linkage使用何种距离度量  n_clusters： 设定聚类算法试图寻找的聚类数量，也就是说有n_clusters个聚类是，聚合的合并才结束

神经网络
	为神经网络预处理数据
		一般神经网络的参数会被初始化(或者说被创建)为一些小的随机数。如果特征值比参数大很多，神经网络往往表现的不如人意。另外，观察值的特征值经过这些神经元的传递后，会进行相加，所以让所有的特征值拥有同样的单位就很重要
		所以，最佳实践就是标准化每个特征值，使其均值为0， 标准差为1

	设计一个神经网络
		神经网络是由多种神经元组成的，神经元层的类型以及他们组成神经网络的方式方式非常多。
		在Keras中构建一个前馈神经网络，需要对网络架构和训练过程做许多选择
			1. 接收一些输入
			2. 给每个输入乘以一个参数作为权重
			3. 对所有加权过的输入求和，再加上偏差(一般是1)
			4. 接下来，将这个值应用到某个函数上(激活函数)
			5. 把输出传递给下一层神经元
		第一： 对隐藏层和输出层中的每一层，必须定义神经元的数量和他的激活函数。总的来说，一个层中神经元越多，神经网络就越能学习复杂的模式。但是，神经元过多可能会使神经网络对训练数据过拟合，影响其在测试数据上的表现
		      对隐藏层来说，一个流行的激活函数是矫正是矫正的线性单元： f(z) = max(0, z) z是加权过的输入和偏差之和。如果z大于0， 激活函数就返回z值。
		第二： 需要决定神经网络总隐藏层的数量。层数越多，神经网络能学到的关系就越复杂，但是计算机开销也就越大
		第三： 必须决定输出层的激活函数(如果有的话)的结构。输出函数的本质经常由神经网络的目标决定。一些常见的输出层的模式：
				二元分类：一个有sigmoid激活函数的神经元
				多元分类：K个神经元(这里K是目标分类的个数)和一个softmax激活函数
				回归：一个没有激活函数的神经元
		第四： 需要定义一个损失函数(用来衡量预测值和真实值的符合程度)。这个函数也经常是由问题的类型所决定的
				二元分类: 二分交叉熵
				多元分类：分类交叉熵
				回归：均方误差
		第五： 需要定义一个优化器，可以被直观地理解为定义的策略绕过了损失函数，并且找到了产生最小误差的那些参数值。常用的优化器有：随机梯度下降，动量随机梯度下降、均方根传播和自适应矩估计
		第六： 可以选择一个或多个指标来评估神经网络的性能，比如准确率

		keras提供了两种创年神经网络的方法：
			1. keras的sequential模型通过把神经元堆叠起来以创建神经网络。
			2. 函数式API，但是这种方法更适合研究人员使用而不适合商用


	训练一个二元分类器

	减少过拟合的方式：
		通过权重调节减少过拟合
		通过提前结束减少过拟合
		通过Dropout减少过拟合
		







